{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Check the data types of the columns. Get the numeric data into dataframe called numerical and categorical columns in a dataframe called categoricals. (You can use np.number and np.object to select the numerical data types and categorical data types respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency \n",
    "\n",
    "# Read the file and standardize the columns\n",
    "df = pd.read_csv('files_for_lab/csv_files/marketing_customer_analysis.csv')\n",
    "df.rename(columns = {'EmploymentStatus': 'Employment Status'}, inplace = True)\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "print(df.dtypes)\n",
    "\n",
    "numericals = df.select_dtypes(['float', 'integer'])\n",
    "categoricals = df.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Now we will try to check the normality of the numerical variables visually:\n",
    "\n",
    "##### 2.1. Use seaborn library to construct distribution plots for the numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(numericals['customer lifetime value'])\n",
    "sns.displot(numericals['income'])\n",
    "sns.displot(numericals['monthly premium auto'])\n",
    "sns.displot(numericals['months since last claim'])\n",
    "sns.displot(numericals['months since policy inception'])\n",
    "sns.displot(numericals['number of open complaints'])\n",
    "sns.displot(numericals['number of policies'])\n",
    "sns.displot(numericals['total claim amount']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2. Use Matplotlib to construct histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 8, ncols = 1, figsize = (5, 30))\n",
    "\n",
    "plt.subplot(8,1,1)\n",
    "plt.hist(numericals['months since last claim'], bins = 35)\n",
    "plt.xlabel('months since last claim')\n",
    "plt.ylabel('frequency')\n",
    "\n",
    "plt.subplot(8,1,2)\n",
    "plt.hist(numericals['number of open complaints'], bins = 5)\n",
    "plt.xlabel('number of open complaints')\n",
    "plt.ylabel('frequency')\n",
    "\n",
    "plt.subplot(8,1,3)\n",
    "plt.hist(numericals['customer lifetime value'], bins = 10)\n",
    "plt.xlabel('customer lifetime value')\n",
    "plt.ylabel('frequency')\n",
    "\n",
    "plt.subplot(8,1,4)\n",
    "plt.hist(numericals['income'], bins = 20)\n",
    "plt.xlabel('income')\n",
    "plt.ylabel('frequency')\n",
    "\n",
    "plt.subplot(8,1,5)\n",
    "plt.hist(numericals['monthly premium auto'], bins = 20)\n",
    "plt.xlabel('monthly premium auto')\n",
    "plt.ylabel('frequency')\n",
    "\n",
    "plt.subplot(8,1,6)\n",
    "plt.hist(numericals['number of policies'], bins = 8)\n",
    "plt.xlabel('number of policies')\n",
    "plt.ylabel('frequency')\n",
    "\n",
    "plt.subplot(8,1,7)\n",
    "plt.hist(numericals['total claim amount'], bins = 100)\n",
    "plt.xlabel('total claim amount')\n",
    "plt.ylabel('frequency')\n",
    "\n",
    "plt.subplot(8,1,8)\n",
    "plt.hist(numericals['months since policy inception'], bins = 100)\n",
    "plt.xlabel('months since policy inception')\n",
    "plt.ylabel('frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3. Do the distributions for different numerical variables look like a normal distribution?\n",
    "\n",
    "The majority of the distributions seem be logarithmic, except for the total claim amount.\n",
    "\n",
    "#### 3. For the numerical variables, check the multicollinearity between the features. Please note that we will use the column `total_claim_amount` later as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pearson correlation matrix to find collinearity\n",
    "\n",
    "# Create correlation matrix\n",
    "corr = numericals.corr()\n",
    "\n",
    "# Set up the matplotlib plot configuration\n",
    "fig, ax = plt.subplots(figsize = (12, 10))\n",
    "\n",
    "# Generate a mask for upper traingle\n",
    "mask = np.triu(np.ones_like(corr, dtype = bool))\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(corr, annot = True, mask = mask)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no obvious strong correlation between the numerical features, so we can move forward and perform the Chi test for the `state` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(categoricals.columns)\n",
    "\n",
    "# state & response:\n",
    "st_resp = pd.crosstab(categoricals['state'], categoricals['response'])\n",
    "chi2, p, dof, array = chi2_contingency(st_resp)\n",
    "print(\"1.\", p)\n",
    "\n",
    "# state & coverage:\n",
    "st_resp = pd.crosstab(categoricals['state'], categoricals['coverage'])\n",
    "chi2, p, dof, array = chi2_contingency(st_resp)\n",
    "print(\"2.\", p)\n",
    "\n",
    "# state & education:\n",
    "st_resp = pd.crosstab(categoricals['state'], categoricals['education'])\n",
    "chi2, p, dof, array = chi2_contingency(st_resp)\n",
    "print(\"3.\", p)\n",
    "\n",
    "# state & employment status:\n",
    "st_resp = pd.crosstab(categoricals['state'], categoricals['employment status'])\n",
    "chi2, p, dof, array = chi2_contingency(st_resp)\n",
    "print(\"4.\", p)\n",
    "\n",
    "# state & gender:\n",
    "st_resp = pd.crosstab(categoricals['state'], categoricals['gender'])\n",
    "chi2, p, dof, array = chi2_contingency(st_resp)\n",
    "print(\"5.\", p)\n",
    "\n",
    "# state & location code:\n",
    "st_resp = pd.crosstab(categoricals['state'], categoricals['location code'])\n",
    "chi2, p, dof, array = chi2_contingency(st_resp)\n",
    "print(\"6.\", p)\n",
    "\n",
    "# state & marital status:\n",
    "st_resp = pd.crosstab(categoricals['state'], categoricals['marital status'])\n",
    "chi2, p, dof, array = chi2_contingency(st_resp)\n",
    "print(\"7.\", p)\n",
    "\n",
    "# state & policy type:\n",
    "st_resp = pd.crosstab(categoricals['state'], categoricals['policy type'])\n",
    "chi2, p, dof, array = chi2_contingency(st_resp)\n",
    "print(\"8.\", p)\n",
    "\n",
    "# state & policy:\n",
    "st_resp = pd.crosstab(categoricals['state'], categoricals['policy'])\n",
    "chi2, p, dof, array = chi2_contingency(st_resp)\n",
    "print(\"9.\", p)\n",
    "\n",
    "# state & renew offer type:\n",
    "st_resp = pd.crosstab(categoricals['state'], categoricals['renew offer type'])\n",
    "chi2, p, dof, array = chi2_contingency(st_resp)\n",
    "print(\"10.\", p)\n",
    "\n",
    "# state & sales channel:\n",
    "st_resp = pd.crosstab(categoricals['state'], categoricals['sales channel'])\n",
    "chi2, p, dof, array = chi2_contingency(st_resp)\n",
    "print(\"11.\", p)\n",
    "\n",
    "# state & vehicle size:\n",
    "st_resp = pd.crosstab(categoricals['state'], categoricals['vehicle size'])\n",
    "chi2, p, dof, array = chi2_contingency(st_resp)\n",
    "print(\"12.\", p)\n",
    "\n",
    "# state & vehicle class:\n",
    "st_resp = pd.crosstab(categoricals['state'], categoricals['vehicle class'])\n",
    "chi2, p, dof, array = chi2_contingency(st_resp)\n",
    "print(\"13.\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the policy & policy type are strongly correlated with the state (possibly because there are specific policies for each state), so we reject the null hypothesis for those.\n",
    "\n",
    "We can also do more targeted tests, e.g. for vehicle class & vehicle size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vehicle class & vehicle size\n",
    "st_resp = pd.crosstab(categoricals['vehicle size'], categoricals['vehicle class'])\n",
    "chi2, p, dof, array = chi2_contingency(st_resp)\n",
    "print(\"1.\", p)\n",
    "\n",
    "# policy & policy type\n",
    "st_resp = pd.crosstab(categoricals['policy'], categoricals['policy type'])\n",
    "chi2, p, dof, array = chi2_contingency(st_resp)\n",
    "print(\"2.\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Drop one of the two features that show a high correlation between them (greater than 0.9). Write code for both the correlation matrix and for seaborn heatmap. If there is no pair of features that have a high correlation, then do not drop any features.\n",
    "\n",
    "We can reject the null-hypothesis for the pairs we saw above so we can drop any of the columns from that pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the columns with the most detail, i.e. policy type & vehicle class\n",
    "df.drop(['policy', 'vehicle size'], axis = 1, inplace = True)\n",
    "\n",
    "# Check columns were dropped\n",
    "print(df.columns)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
